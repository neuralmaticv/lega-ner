{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacija prepoznavanja imenovanih entiteta\n",
    "\n",
    "**Zadatak:** fine-tuning Transformer modela za prepoznavanje imenovanih entiteta (NER) na srpskom jeziku.  \n",
    "**Skup podataka:** [COMtext.SR.legal](https://raw.githubusercontent.com/ICEF-NLP/COMtext.SR/ee8c2432fb4229012a3cb396b7823639216fc3da/data/comtext.sr.legal.ijekavica.conllu)  \n",
    "**Modeli:** BERTić i SrBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import cyrtranslit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION =====\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "MODE = \"cv\"  # \"standard\" or \"cv\" (cross-validation)\n",
    "\n",
    "# Dataset file path\n",
    "FILE_PATH = DATA_DIR / \"comtext.sr.legal.ekavica.conllu\"\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"BERTić\": {\n",
    "        \"name\": \"classla/bcms-bertic\",\n",
    "        \"display_name\": \"BERTić-COMtext-SR-legal-NER-ijekavica\",\n",
    "        \"use_cyrillic\": False,\n",
    "    },\n",
    "    \"SrBERTa\": {\n",
    "        \"name\": \"nemanjaPetrovic/SrBERTa\",\n",
    "        \"display_name\": \"SrBERTa\",\n",
    "        \"use_cyrillic\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Training hyperparameters (matching reference config)\n",
    "SEED = 64\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 4e-5\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 100\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.0\n",
    "NUM_FOLDS = 10\n",
    "TEST_SIZE = 0.1 # 10% test size for standard mode - not used in CV mode\n",
    "USE_EARLY_STOPPING = False\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / \"logs\").mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / \"models\").mkdir(exist_ok=True)\n",
    "\n",
    "ENTITY_TYPES_DISPLAY = [\n",
    "    \"PER\",\n",
    "    \"LOC\",\n",
    "    \"ADR\",\n",
    "    \"COURT\",\n",
    "    \"INST\",\n",
    "    \"COM\",\n",
    "    \"OTHORG\",\n",
    "    \"LAW\",\n",
    "    \"REF\",\n",
    "    \"IDPER\",\n",
    "    \"IDCOM\",\n",
    "    \"IDTAX\",\n",
    "    \"NUMACC\",\n",
    "    \"NUMDOC\",\n",
    "    \"NUMCAR\",\n",
    "    \"NUMPLOT\",\n",
    "    \"IDOTH\",\n",
    "    \"CONTACT\",\n",
    "    \"DATE\",\n",
    "    \"MONEY\",\n",
    "    \"MISC\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA LOADING FUNCTION =====\n",
    "def load_corpus_sentences(\n",
    "    filepath_list: list[Path], use_cyrillic: bool = False, conllup: bool = False\n",
    ") -> tuple[list[dict], set]:\n",
    "    \"\"\"\n",
    "    Parses CoNLL-U files into a list of sentence dictionaries.\n",
    "    Ensures tokens are grouped by sentence to preserve context.\n",
    "\n",
    "    Args:\n",
    "        filepath_list: List of paths to CoNLL-U files\n",
    "        use_cyrillic: If True, transliterate to Cyrillic (for SrBERTa)\n",
    "        conllup: If True, use column 4 for NER tag, else column 3\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (dataset, wordlist)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    wordlist = set()\n",
    "    global_sent_index = 0\n",
    "\n",
    "    for filepath in filepath_list:\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "\n",
    "        raw_sentences = content.split(\"\\n\\n\")\n",
    "\n",
    "        for raw_sent in raw_sentences:\n",
    "            if not raw_sent.strip():\n",
    "                continue\n",
    "\n",
    "            sentence_id_str = \"\"\n",
    "            document_id = None\n",
    "            tokens = []\n",
    "            labels = []\n",
    "\n",
    "            lines = raw_sent.split(\"\\n\")\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"#\"):\n",
    "                    if \"sent_id\" in line:\n",
    "                        sentence_id_str = line.split(\"=\")[1].strip()\n",
    "                    elif \"newdoc id\" in line:\n",
    "                        document_id = line.split(\"=\")[1].strip()\n",
    "                    continue\n",
    "\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                token = parts[1]\n",
    "                lemma = parts[2] if len(parts) > 2 else token\n",
    "\n",
    "                if not conllup:\n",
    "                    tag = parts[3] if len(parts) > 3 else \"O\"\n",
    "                else:\n",
    "                    try:\n",
    "                        tag = parts[4]\n",
    "                    except IndexError:\n",
    "                        tag = \"O\"\n",
    "\n",
    "                # Transliteration for SrBERTa (Cyrillic)\n",
    "                if use_cyrillic:\n",
    "                    token = cyrtranslit.to_cyrillic(token, \"sr\")\n",
    "                    lemma = cyrtranslit.to_cyrillic(lemma, \"sr\")\n",
    "\n",
    "                tokens.append(token)\n",
    "                labels.append(tag)\n",
    "                wordlist.add(token)\n",
    "\n",
    "            if len(tokens) > 0:\n",
    "                dataset.append(\n",
    "                    {\n",
    "                        \"sentence_id\": global_sent_index,\n",
    "                        \"conllu_id\": sentence_id_str,\n",
    "                        \"document_id\": document_id,\n",
    "                        \"words\": tokens,\n",
    "                        \"labels\": labels,\n",
    "                    }\n",
    "                )\n",
    "                global_sent_index += 1\n",
    "\n",
    "    return dataset, wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TOKENIZATION & ALIGNMENT FUNCTION =====\n",
    "def tokenize_and_align_labels(examples: dict, tokenizer, label2id: dict, max_length: int = 512) -> dict:\n",
    "    \"\"\"\n",
    "    Tokenizes sentences and aligns labels with sub-word tokens.\n",
    "    Assigns -100 to special tokens and subsequent sub-word pieces.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True, max_length=max_length)\n",
    "\n",
    "    labels = []\n",
    "    for i, label_seq in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label_seq[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EVALUATION FUNCTIONS =====\n",
    "def strip_bio_prefix(tag: str) -> str:\n",
    "    \"\"\"Remove B- or I- prefix from a tag.\"\"\"\n",
    "    if tag.startswith(\"B-\") or tag.startswith(\"I-\"):\n",
    "        return tag[2:]\n",
    "    return tag\n",
    "\n",
    "\n",
    "def compute_metrics_default(true_labels: list[list[str]], pred_labels: list[list[str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Compute metrics with default (type-level) matching.\n",
    "    BIO prefixes are ignored - only entity type matters.\n",
    "\n",
    "    Returns:\n",
    "        Dict with accuracy, macro_f1_with_o, macro_f1_without_o, and per-class F1\n",
    "    \"\"\"\n",
    "    # Convert to type-level (strip BIO prefixes)\n",
    "    true_type = [[strip_bio_prefix(t) for t in seq] for seq in true_labels]\n",
    "    pred_type = [[strip_bio_prefix(p) for p in seq] for seq in pred_labels]\n",
    "\n",
    "    # Flatten for metrics calculation\n",
    "    true_flat = [t for seq in true_type for t in seq]\n",
    "    pred_flat = [p for seq in pred_type for p in seq]\n",
    "\n",
    "    # Get unique classes\n",
    "    all_classes = sorted(set(true_flat) | set(pred_flat))\n",
    "\n",
    "    # Accuracy\n",
    "    correct = sum(1 for t, p in zip(true_flat, pred_flat) if t == p)\n",
    "    accuracy = correct / len(true_flat) if true_flat else 0.0\n",
    "\n",
    "    # Per-class metrics\n",
    "    class_metrics = {}\n",
    "    for cls in all_classes:\n",
    "        tp = sum(1 for t, p in zip(true_flat, pred_flat) if t == cls and p == cls)\n",
    "        fp = sum(1 for t, p in zip(true_flat, pred_flat) if t != cls and p == cls)\n",
    "        fn = sum(1 for t, p in zip(true_flat, pred_flat) if t == cls and p != cls)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        class_metrics[cls] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"support\": sum(1 for t in true_flat if t == cls),\n",
    "        }\n",
    "\n",
    "    # Macro F1 with O\n",
    "    f1_scores_with_o = [class_metrics[cls][\"f1\"] for cls in all_classes if cls in class_metrics]\n",
    "    macro_f1_with_o = np.mean(f1_scores_with_o) if f1_scores_with_o else 0.0\n",
    "\n",
    "    # Macro F1 without O\n",
    "    f1_scores_without_o = [class_metrics[cls][\"f1\"] for cls in all_classes if cls != \"O\" and cls in class_metrics]\n",
    "    macro_f1_without_o = np.mean(f1_scores_without_o) if f1_scores_without_o else 0.0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1_with_o\": macro_f1_with_o,\n",
    "        \"macro_f1_without_o\": macro_f1_without_o,\n",
    "        \"per_class\": class_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics_strict(true_labels: list[list[str]], pred_labels: list[list[str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Compute metrics with strict (exact) matching.\n",
    "    Full tag strings must match exactly (B-PER != I-PER).\n",
    "    Reports B- and I- tags separately.\n",
    "\n",
    "    Returns:\n",
    "        Dict with accuracy, macro_f1_with_o, macro_f1_without_o, and per-class F1 (B/I separate)\n",
    "    \"\"\"\n",
    "    # Flatten for metrics calculation\n",
    "    true_flat = [t for seq in true_labels for t in seq]\n",
    "    pred_flat = [p for seq in pred_labels for p in seq]\n",
    "\n",
    "    # Get unique classes\n",
    "    all_classes = sorted(set(true_flat) | set(pred_flat))\n",
    "\n",
    "    # Accuracy\n",
    "    correct = sum(1 for t, p in zip(true_flat, pred_flat) if t == p)\n",
    "    accuracy = correct / len(true_flat) if true_flat else 0.0\n",
    "\n",
    "    # Per-class metrics (exact tag matching)\n",
    "    class_metrics = {}\n",
    "    for cls in all_classes:\n",
    "        tp = sum(1 for t, p in zip(true_flat, pred_flat) if t == cls and p == cls)\n",
    "        fp = sum(1 for t, p in zip(true_flat, pred_flat) if t != cls and p == cls)\n",
    "        fn = sum(1 for t, p in zip(true_flat, pred_flat) if t == cls and p != cls)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        class_metrics[cls] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"support\": sum(1 for t in true_flat if t == cls),\n",
    "        }\n",
    "\n",
    "    # Macro F1 with O\n",
    "    f1_scores_with_o = [class_metrics[cls][\"f1\"] for cls in all_classes]\n",
    "    macro_f1_with_o = np.mean(f1_scores_with_o) if f1_scores_with_o else 0.0\n",
    "\n",
    "    # Macro F1 without O\n",
    "    f1_scores_without_o = [class_metrics[cls][\"f1\"] for cls in all_classes if cls != \"O\"]\n",
    "    macro_f1_without_o = np.mean(f1_scores_without_o) if f1_scores_without_o else 0.0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1_with_o\": macro_f1_with_o,\n",
    "        \"macro_f1_without_o\": macro_f1_without_o,\n",
    "        \"per_class\": class_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_trainer_metrics(eval_preds, id2label: dict) -> dict:\n",
    "    \"\"\"Compute metrics for Trainer callback during training.\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels_list = []\n",
    "    pred_labels_list = []\n",
    "\n",
    "    for i, label_seq in enumerate(labels):\n",
    "        true_seq = []\n",
    "        pred_seq = []\n",
    "        for j, label_id in enumerate(label_seq):\n",
    "            if label_id != -100:\n",
    "                true_seq.append(id2label[label_id])\n",
    "                pred_seq.append(id2label[preds[i][j]])\n",
    "        true_labels_list.append(true_seq)\n",
    "        pred_labels_list.append(pred_seq)\n",
    "\n",
    "    # Use default (type-level) F1 for model selection\n",
    "    metrics = compute_metrics_default(true_labels_list, pred_labels_list)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"f1\": metrics[\"macro_f1_without_o\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINING FUNCTION =====\n",
    "def train_and_evaluate(\n",
    "    train_data: list[dict],\n",
    "    test_data: list[dict],\n",
    "    model_config: dict,\n",
    "    label2id: dict,\n",
    "    id2label: dict,\n",
    "    output_base: Path,\n",
    "    fold_num: int | None = None,\n",
    ") -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Train a model and evaluate using both default and strict settings.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (default_metrics, strict_metrics)\n",
    "    \"\"\"\n",
    "    model_name = model_config[\"name\"]\n",
    "\n",
    "    fold_suffix = f\"_fold_{fold_num}\" if fold_num is not None else \"\"\n",
    "    output_path = output_base / f\"{model_name.replace('/', '_')}{fold_suffix}\"\n",
    "\n",
    "    print(f\"  Loading tokenizer: {model_name}\")\n",
    "    # RoBERTa-based models (like SrBERTa) need add_prefix_space=True for pre-tokenized inputs\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_prefix_space=True)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    # Create HuggingFace datasets\n",
    "    hf_train = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "    hf_test = Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "\n",
    "    print(\"  Tokenizing datasets...\")\n",
    "    tokenized_train = hf_train.map(\n",
    "        tokenize_and_align_labels, batched=True, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id}\n",
    "    )\n",
    "    tokenized_test = hf_test.map(\n",
    "        tokenize_and_align_labels, batched=True, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id}\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    print(f\"  Loading model: {model_name}\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    # Training arguments\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(output_path),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=False,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0,\n",
    "        logging_dir=str(OUTPUT_DIR / \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=True,\n",
    "        save_total_limit=2,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Create compute_metrics with id2label in closure\n",
    "    def get_compute_metrics(id2label_map):\n",
    "        def _compute_metrics(eval_preds):\n",
    "            return compute_trainer_metrics(eval_preds, id2label_map)\n",
    "\n",
    "        return _compute_metrics\n",
    "\n",
    "    # Setup callbacks based on configuration\n",
    "    callbacks = []\n",
    "    if USE_EARLY_STOPPING:\n",
    "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=get_compute_metrics(id2label),\n",
    "        callbacks=callbacks,  # empty list when early stopping disabled\n",
    "    )\n",
    "\n",
    "    print(\"  Training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Generate predictions\n",
    "    print(\"  Generating predictions...\")\n",
    "    predictions, labels, _ = trainer.predict(tokenized_test)\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Convert IDs back to tags\n",
    "    true_labels_list = []\n",
    "    pred_labels_list = []\n",
    "\n",
    "    for i, label_seq in enumerate(labels):\n",
    "        true_seq = []\n",
    "        pred_seq = []\n",
    "        for j, label_id in enumerate(label_seq):\n",
    "            if label_id != -100:\n",
    "                true_seq.append(id2label[label_id])\n",
    "                pred_seq.append(id2label[preds[i][j]])\n",
    "        true_labels_list.append(true_seq)\n",
    "        pred_labels_list.append(pred_seq)\n",
    "\n",
    "    # Compute both evaluation settings\n",
    "    default_metrics = compute_metrics_default(true_labels_list, pred_labels_list)\n",
    "    strict_metrics = compute_metrics_strict(true_labels_list, pred_labels_list)\n",
    "\n",
    "    # Cleanup checkpoints\n",
    "    if output_path.exists():\n",
    "        for checkpoint_dir in output_path.glob(\"checkpoint-*\"):\n",
    "            shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
    "\n",
    "    return default_metrics, strict_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results: dict[str, dict[str, dict]], output_path: Path, mode: str = \"cv\") -> None:\n",
    "    \"\"\"\n",
    "    Save results to CSV file.\n",
    "\n",
    "    Args:\n",
    "        results: Dict with structure {model_key: {\"default\": metrics, \"strict\": metrics}}\n",
    "        output_path: Path to save CSV file\n",
    "        mode: Training mode (cv or standard)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for model_key, model_results in results.items():\n",
    "        model_display = MODELS[model_key][\"display_name\"]\n",
    "\n",
    "        for eval_type in [\"default\", \"strict\"]:\n",
    "            metrics = model_results.get(eval_type, {})\n",
    "            if not metrics:\n",
    "                continue\n",
    "\n",
    "            # Base row with overall metrics\n",
    "            base_row = {\n",
    "                \"model\": model_key,\n",
    "                \"model_display\": model_display,\n",
    "                \"eval_type\": eval_type,\n",
    "                \"mode\": mode,\n",
    "                \"accuracy\": metrics.get(\"accuracy\"),\n",
    "                \"macro_f1_with_o\": metrics.get(\"macro_f1_with_o\"),\n",
    "                \"macro_f1_without_o\": metrics.get(\"macro_f1_without_o\"),\n",
    "            }\n",
    "\n",
    "            # Add per-class F1 scores\n",
    "            per_class = metrics.get(\"per_class\", {})\n",
    "\n",
    "            if eval_type == \"default\":\n",
    "                # For default, add entity type F1 (without B-/I- prefix)\n",
    "                for entity_type in ENTITY_TYPES_DISPLAY + [\"O\"]:\n",
    "                    f1 = per_class.get(entity_type, {}).get(\"f1\")\n",
    "                    base_row[f\"f1_{entity_type}\"] = f1\n",
    "            else:\n",
    "                # For strict, add both B- and I- F1 scores\n",
    "                for entity_type in ENTITY_TYPES_DISPLAY + [\"O\"]:\n",
    "                    if entity_type == \"O\":\n",
    "                        f1 = per_class.get(\"O\", {}).get(\"f1\")\n",
    "                        base_row[\"f1_O\"] = f1\n",
    "                    else:\n",
    "                        b_f1 = per_class.get(f\"B-{entity_type}\", {}).get(\"f1\")\n",
    "                        i_f1 = per_class.get(f\"I-{entity_type}\", {}).get(\"f1\")\n",
    "                        base_row[f\"f1_B-{entity_type}\"] = b_f1\n",
    "                        base_row[f\"f1_I-{entity_type}\"] = i_f1\n",
    "\n",
    "            rows.append(base_row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"  Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RESULTS AGGREGATION =====\n",
    "def aggregate_cv_results(fold_results: list[tuple[dict, dict]]) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Aggregate results from multiple CV folds.\n",
    "\n",
    "    Args:\n",
    "        fold_results: List of (default_metrics, strict_metrics) tuples\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (aggregated_default, aggregated_strict)\n",
    "    \"\"\"\n",
    "\n",
    "    def aggregate_metrics(metrics_list: list[dict]) -> dict:\n",
    "        # Aggregate scalar metrics\n",
    "        result = {\n",
    "            \"accuracy\": np.mean([m[\"accuracy\"] for m in metrics_list]),\n",
    "            \"macro_f1_with_o\": np.mean([m[\"macro_f1_with_o\"] for m in metrics_list]),\n",
    "            \"macro_f1_without_o\": np.mean([m[\"macro_f1_without_o\"] for m in metrics_list]),\n",
    "            \"per_class\": {},\n",
    "        }\n",
    "\n",
    "        # Aggregate per-class metrics\n",
    "        all_classes = set()\n",
    "        for m in metrics_list:\n",
    "            all_classes.update(m[\"per_class\"].keys())\n",
    "\n",
    "        for cls in all_classes:\n",
    "            f1_scores = [m[\"per_class\"][cls][\"f1\"] for m in metrics_list if cls in m[\"per_class\"]]\n",
    "            if f1_scores:\n",
    "                precision_scores = [m[\"per_class\"][cls][\"precision\"] for m in metrics_list if cls in m[\"per_class\"]]\n",
    "                recall_scores = [m[\"per_class\"][cls][\"recall\"] for m in metrics_list if cls in m[\"per_class\"]]\n",
    "                result[\"per_class\"][cls] = {\n",
    "                    \"f1\": np.mean(f1_scores),\n",
    "                    \"precision\": np.mean(precision_scores),\n",
    "                    \"recall\": np.mean(recall_scores),\n",
    "                }\n",
    "\n",
    "        return result\n",
    "\n",
    "    default_list = [r[0] for r in fold_results]\n",
    "    strict_list = [r[1] for r in fold_results]\n",
    "\n",
    "    return aggregate_metrics(default_list), aggregate_metrics(strict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standard_mode(sentences_by_script: dict[bool, list[dict]], label2id: dict, id2label: dict) -> dict:\n",
    "    \"\"\"Run standard train/test split mode with sequential (non-shuffled) split.\n",
    "\n",
    "    Args:\n",
    "        sentences_by_script: Dict with keys False (Latin) and True (Cyrillic) containing pre-loaded sentences\n",
    "        label2id: Label to ID mapping\n",
    "        id2label: ID to label mapping\n",
    "    \"\"\"\n",
    "    print(\"STANDARD MODE: Single train/test split (sequential)\")\n",
    "\n",
    "    # Use Latin script sentences for splitting (both models have same sentence order)\n",
    "    base_sentences = sentences_by_script[False]\n",
    "\n",
    "    # Sequential split (no shuffle)\n",
    "    train_data, test_data = train_test_split(base_sentences, test_size=TEST_SIZE, random_state=SEED, shuffle=False)\n",
    "    print(f\"Train: {len(train_data)} sentences, Test: {len(test_data)} sentences\")\n",
    "    print(\"  (Using sequential split - no shuffle)\")\n",
    "\n",
    "    # Get train/test indices\n",
    "    train_indices = [s[\"sentence_id\"] for s in train_data]\n",
    "    test_indices = [s[\"sentence_id\"] for s in test_data]\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_base = OUTPUT_DIR / \"models\" / f\"standard_{timestamp}\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_key, model_config in MODELS.items():\n",
    "        print(f\"\\n{'=' * 40}\")\n",
    "        print(f\"Training {model_key}\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Use pre-loaded sentences with appropriate script\n",
    "        model_sentences = sentences_by_script[model_config[\"use_cyrillic\"]]\n",
    "\n",
    "        # Use same indices for consistency\n",
    "        model_train = [model_sentences[i] for i in train_indices]\n",
    "        model_test = [model_sentences[i] for i in test_indices]\n",
    "\n",
    "        default_metrics, strict_metrics = train_and_evaluate(\n",
    "            model_train,\n",
    "            model_test,\n",
    "            model_config,\n",
    "            label2id,\n",
    "            id2label,\n",
    "            output_base,\n",
    "        )\n",
    "\n",
    "        results[model_key] = {\n",
    "            \"default\": default_metrics,\n",
    "            \"strict\": strict_metrics,\n",
    "        }\n",
    "\n",
    "        csv_path = OUTPUT_DIR / f\"ner_results_standard_{timestamp}.csv\"\n",
    "        save_results_to_csv(results, csv_path, mode=\"standard\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from scipy.fftpack import cs_diff\n",
    "\n",
    "\n",
    "def run_cv_mode(sentences_by_script: dict[bool, list[dict]], label2id: dict, id2label: dict) -> dict:\n",
    "    \"\"\"Run 10-fold cross-validation mode with sequential splits (no shuffle).\n",
    "\n",
    "    Args:\n",
    "        sentences_by_script: Dict with keys False (Latin) and True (Cyrillic) containing pre-loaded sentences\n",
    "        label2id: Label to ID mapping\n",
    "        id2label: ID to label mapping\n",
    "    \"\"\"\n",
    "    print(f\"CROSS-VALIDATION MODE: {NUM_FOLDS}-Fold CV (sequential)\")\n",
    "\n",
    "    # Use Latin script sentences for fold splitting (both models have same sentence order)\n",
    "    base_sentences = sentences_by_script[False]\n",
    "    print(f\"Loaded {len(base_sentences)} sentences\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_base = OUTPUT_DIR / \"models\" / f\"cv_{timestamp}\"\n",
    "\n",
    "    # Sequential splits (no shuffle)\n",
    "    kf = KFold(n_splits=NUM_FOLDS, shuffle=False)\n",
    "    print(\"Using sequential splits (no shuffle)\")\n",
    "    print(\"  (Sequential splits create natural document-level folds)\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_key, model_config in MODELS.items():\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Training {model_key} ({NUM_FOLDS}-Fold CV)\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Use pre-loaded sentences with appropriate script\n",
    "        model_sentences = sentences_by_script[model_config[\"use_cyrillic\"]]\n",
    "\n",
    "        fold_results = []\n",
    "\n",
    "        for fold_num, (train_index, test_index) in enumerate(kf.split(model_sentences), 1):\n",
    "            print(f\"\\n===== FOLD {fold_num}/{NUM_FOLDS} =====\")\n",
    "\n",
    "            # Sequential sentence-level split\n",
    "            train_data = [model_sentences[i] for i in train_index]\n",
    "            test_data = [model_sentences[i] for i in test_index]\n",
    "\n",
    "            print(f\"  Train: {len(train_data)} sentences, Test: {len(test_data)} sentences\")\n",
    "\n",
    "            default_metrics, strict_metrics = train_and_evaluate(\n",
    "                train_data,\n",
    "                test_data,\n",
    "                model_config,\n",
    "                label2id,\n",
    "                id2label,\n",
    "                output_base,\n",
    "                fold_num=fold_num,\n",
    "            )\n",
    "\n",
    "            fold_results.append((default_metrics, strict_metrics))\n",
    "\n",
    "        # Aggregate fold results\n",
    "        agg_default, agg_strict = aggregate_cv_results(fold_results)\n",
    "\n",
    "        results[model_key] = {\n",
    "            \"default\": agg_default,\n",
    "            \"strict\": agg_strict,\n",
    "        }\n",
    "\n",
    "        csv_path = OUTPUT_DIR / f\"ner_results_cv_{timestamp}.csv\"\n",
    "        save_results_to_csv(results, csv_path, mode=\"cv\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(results: dict) -> None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    for model_key, model_results in results.items():\n",
    "        default_m = model_results.get(\"default\", {})\n",
    "        print(f\"{model_key}:\")\n",
    "        print(f\"  Accuracy: {default_m.get('accuracy', 'N/A'):.4f}\")\n",
    "        print(f\"  Macro F1 (with O): {default_m.get('macro_f1_with_o', 'N/A'):.4f}\")\n",
    "        print(f\"  Macro F1 (without O): {default_m.get('macro_f1_without_o', 'N/A'):.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "sentences_latin, _ = load_corpus_sentences([FILE_PATH], use_cyrillic=False, conllup=True)\n",
    "sentences_cyrillic, _ = load_corpus_sentences([FILE_PATH], use_cyrillic=True, conllup=True)\n",
    "\n",
    "sentences_by_script = {\n",
    "    False: sentences_latin,  # for BERTić\n",
    "    True: sentences_cyrillic,  # for SrBERTa\n",
    "}\n",
    "\n",
    "unique_labels = set()\n",
    "for s in sentences_latin:\n",
    "    unique_labels.update(s[\"labels\"])\n",
    "sorted_labels = sorted(list(unique_labels))\n",
    "label2id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "id2label = {idx: label for idx, label in enumerate(sorted_labels)}\n",
    "\n",
    "if MODE == \"standard\":\n",
    "    results = run_standard_mode(sentences_by_script, label2id, id2label)\n",
    "else:\n",
    "    results = run_cv_mode(sentences_by_script, label2id, id2label)\n",
    "\n",
    "print_summary(results)\n",
    "\n",
    "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_json = OUTPUT_DIR / f\"ner_results_{MODE}_{timestamp_str}.json\"\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super().default(obj)\n",
    "\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4, cls=NumpyEncoder)\n",
    "\n",
    "print(f\"\\nFinal JSON results saved to {output_json}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-project (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
