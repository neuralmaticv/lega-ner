{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacija prepoznavanja imenovanih entiteta\n",
    "\n",
    "**Zadatak:** fine-tuning Transformer modela za prepoznavanje imenovanih entiteta (NER) na srpskom jeziku.  \n",
    "**Skup podataka:** [COMtext.SR.legal](https://raw.githubusercontent.com/ICEF-NLP/COMtext.SR/ee8c2432fb4229012a3cb396b7823639216fc3da/data/comtext.sr.legal.ijekavica.conllu)  \n",
    "**Modeli:** BERTić i SrBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available, will use CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Učitavanje i parsiranje CoNLL-U formata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conllu(file_path):\n",
    "    \"\"\"\n",
    "    Parse CoNLL-U format file.\n",
    "    Returns: (sentences, labels) where each is a list of lists.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip comments and blank lines (end of sentence)\n",
    "            if line.startswith(\"#\") or not line:\n",
    "                if current_tokens:\n",
    "                    sentences.append(current_tokens)\n",
    "                    labels.append(current_labels)\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "                continue\n",
    "            \n",
    "            # Parse token line: ID FORM LEMMA POS NER\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 5 and parts[0].isdigit():\n",
    "                token = parts[1]       # Column 2: word form\n",
    "                ner_tag = parts[4]     # Column 5: NER tag\n",
    "                current_tokens.append(token)\n",
    "                current_labels.append(ner_tag)\n",
    "        \n",
    "        # Don't forget last sentence\n",
    "        if current_tokens:\n",
    "            sentences.append(current_tokens)\n",
    "            labels.append(current_labels)\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "# Load data\n",
    "data_path = Path(\"../data/comtext.sr.legal.ijekavica.conllu\")\n",
    "sentences, labels = parse_conllu(data_path)\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sentences\")\n",
    "print(f\"Total tokens: {sum(len(s) for s in sentences)}\")\n",
    "print(\"\\nExample sentence 1:\")\n",
    "print(f\"Tokens: {sentences[0][:10]}...\")\n",
    "print(f\"Labels: {labels[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza distribucije labela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set()\n",
    "for label_seq in labels:\n",
    "    all_labels.update(label_seq)\n",
    "\n",
    "unique_labels = sorted(list(all_labels))\n",
    "print(f\"Total unique labels: {len(unique_labels)}\")\n",
    "print(f\"\\nAll labels:\\n{unique_labels}\")\n",
    "\n",
    "# Count occurrences\n",
    "label_counts = {}\n",
    "for label_seq in labels:\n",
    "    for label in label_seq:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "# Show top 10 most frequent\n",
    "print(\"\\nTop 10 most frequent labels:\")\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  {label:8}: {count:6,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreiranje mapiranja labela za model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Created mappings for {len(label2id)} labels\")\n",
    "print(\"\\nFirst 10 label mappings:\")\n",
    "for label, idx in list(label2id.items())[:10]:\n",
    "    print(f\"  {label:10s} -> {idx}\")\n",
    "\n",
    "print(f\"\\nTest mapping: 'B-PER' -> {label2id['B-PER']} -> '{id2label[label2id['B-PER']]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pomoćne funkcije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, label2id, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize text and align labels with subword tokens.\n",
    "    \n",
    "    Args:\n",
    "        examples: Dict with 'tokens' and 'ner_tags' keys\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        label2id: Label to ID mapping\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized inputs with aligned labels\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens (CLS, SEP, PAD) get -100\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # First subword of a word gets the label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label_seq[word_idx]])\n",
    "            # Subsequent subwords get -100 (ignored)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def strip_bio_prefix(labels):\n",
    "    \"\"\"Convert B-PER, I-PER → PER (entity type only)\"\"\"\n",
    "    stripped = []\n",
    "    for label in labels:\n",
    "        if label == 'O':\n",
    "            stripped.append('O')\n",
    "        else:\n",
    "            # Remove B- or I- prefix\n",
    "            entity_type = label.split('-', 1)[1] if '-' in label else label\n",
    "            stripped.append(entity_type)\n",
    "    return stripped\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for model predictions.\n",
    "    This gets called automatically during evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = pred\n",
    "    \n",
    "    # Get predicted label IDs (argmax over logits)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Flatten and remove ignored indices (-100)\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] != -100:\n",
    "                true_labels.append(id2label[labels[i][j]])\n",
    "                pred_labels.append(id2label[predictions[i][j]])\n",
    "    \n",
    "    # Convert to arrays\n",
    "    y_true = np.array(true_labels)\n",
    "    y_pred = np.array(pred_labels)\n",
    "    \n",
    "    # DEFAULT EVALUATION (entity type only)\n",
    "    y_true_default = strip_bio_prefix(y_true)\n",
    "    y_pred_default = strip_bio_prefix(y_pred)\n",
    "    \n",
    "    default_acc = accuracy_score(y_true_default, y_pred_default)\n",
    "    \n",
    "    unique_labels_default = sorted(set(y_true_default) | set(y_pred_default))\n",
    "    entity_labels_default = [l for l in unique_labels_default if l != 'O']\n",
    "    \n",
    "    default_f1_with_o = f1_score(y_true_default, y_pred_default, labels=unique_labels_default, average='macro', zero_division=0)\n",
    "    default_f1_without_o = f1_score(y_true_default, y_pred_default, labels=entity_labels_default, average='macro', zero_division=0)\n",
    "    \n",
    "    # STRICT EVALUATION (full BIO tags)\n",
    "    strict_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    unique_labels = sorted(set(y_true) | set(y_pred))\n",
    "    entity_labels = [l for l in unique_labels if l != 'O']\n",
    "    \n",
    "    strict_f1_with_o = f1_score(y_true, y_pred, labels=unique_labels, average='macro', zero_division=0)\n",
    "    strict_f1_without_o = f1_score(y_true, y_pred, labels=entity_labels, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        # Default mode\n",
    "        'default_accuracy': default_acc,\n",
    "        'default_f1_with_o': default_f1_with_o,\n",
    "        'default_f1_without_o': default_f1_without_o,\n",
    "\n",
    "        # Strict mode\n",
    "        'strict_accuracy': strict_acc,\n",
    "        'strict_f1_with_o': strict_f1_with_o,\n",
    "        'strict_f1_without_o': strict_f1_without_o,\n",
    "    }\n",
    "\n",
    "# Create sentence-level stratification labels\n",
    "# Use dominant entity type per sentence\n",
    "def get_sentence_label(ner_tags):\n",
    "    entity_types = [tag.split('-')[-1] for tag in ner_tags]\n",
    "    entities = [e for e in entity_types if e != 'O']\n",
    "    if not entities:\n",
    "        return 'O'\n",
    "    return Counter(entities).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross-validation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "SEED = 64\n",
    "sentence_labels = [get_sentence_label(tags) for tags in labels]\n",
    "\n",
    "# Create folds\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "fold_splits = list(enumerate(skf.split(sentences, sentence_labels), 1))\n",
    "\n",
    "print(f\"Created {N_FOLDS} stratified folds:\")\n",
    "for fold_num, (train_idx, eval_idx) in fold_splits:\n",
    "    print(f\"  Fold {fold_num}: Train={len(train_idx)}, Eval={len(eval_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treniranje i evaluacija modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"classla/bcms-bertic\"\n",
    "model_name = \"nemanjaPetrovic/SrBERTa\"\n",
    "model_short_name = model_name.split('/')[-1]\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_base = f\"../outputs/models/{model_short_name}_10fold_{timestamp}\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "all_fold_results = []\n",
    "\n",
    "print(f\"Starting training and evaluation across {N_FOLDS} folds...\")\n",
    "print(\"=\" * 70)\n",
    "for fold_num, (train_idx, eval_idx) in fold_splits:\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"tokens\": [sentences[i] for i in train_idx],\n",
    "        \"ner_tags\": [labels[i] for i in train_idx],\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"tokens\": [sentences[i] for i in eval_idx],\n",
    "        \"ner_tags\": [labels[i] for i in eval_idx],\n",
    "    })\n",
    "\n",
    "    print(f\"Loading tokenizer from {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        add_prefix_space=True,  # for RoBERTa-based models\n",
    "    )\n",
    "\n",
    "    print(f\"Loading model from {model_name}...\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    tokenized_train = train_dataset.map(\n",
    "        lambda x: tokenize_and_align_labels(x, tokenizer, label2id),\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "    tokenized_eval = eval_dataset.map(\n",
    "        lambda x: tokenize_and_align_labels(x, tokenizer, label2id),\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= f\"{output_base}/fold_{fold_num}\",\n",
    "\n",
    "        # Training schedule\n",
    "        num_train_epochs=20,\n",
    "        per_device_train_batch_size=8, # used in original implementation\n",
    "        per_device_eval_batch_size=256, # original 100\n",
    "        \n",
    "        # Optimization\n",
    "        learning_rate=4e-05,\n",
    "        weight_decay=0,\n",
    "        warmup_ratio=0.06,\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"strict_f1_without_o\",\n",
    "        greater_is_better=True,\n",
    "        \n",
    "        # Performance\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # Logging\n",
    "        logging_dir=f\"../outputs/logs/{timestamp}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "        \n",
    "        # Checkpointing\n",
    "        save_total_limit=2,\n",
    "        \n",
    "        # Reproducibility\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"Training fold {fold_num}/{N_FOLDS}...\")\n",
    "    trainer.train()\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "\n",
    "    fold_results = {\n",
    "        \"fold_num\": fold_num,\n",
    "        \"train_samples\": len(train_idx),\n",
    "        \"eval_samples\": len(eval_idx),\n",
    "        **eval_metrics,\n",
    "    }\n",
    "    all_fold_results.append(fold_results)\n",
    "\n",
    "    # print(f\"\\nFold {fold_num} results:\")\n",
    "    # print(\"DEFAULT EVALUATION (entity type only):\")\n",
    "    # print(f\"  Accuracy:           {eval_metrics['eval_default_accuracy']:.4f}\")\n",
    "    # print(f\"  F1-Macro (with O):  {eval_metrics['eval_default_f1_with_o']:.4f}\")\n",
    "    # print(f\"  F1-Macro (no O):    {eval_metrics['eval_default_f1_without_o']:.4f}\")\n",
    "    # print()\n",
    "    # print(\"STRICT EVALUATION (full BIO tags):\")\n",
    "    # print(f\"  Accuracy:           {eval_metrics['eval_strict_accuracy']:.4f}\")\n",
    "    # print(f\"  F1-Macro (with O):  {eval_metrics['eval_strict_f1_with_o']:.4f}\")\n",
    "    # print(f\"  F1-Macro (no O):    {eval_metrics['eval_strict_f1_without_o']:.4f}\")\n",
    "    # print(\"=\" * 70)\n",
    "\n",
    "    # cleanup\n",
    "    del trainer, model, tokenizer, tokenized_train, tokenized_eval\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacija rezultata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Aggregate results\n",
    "results_df = pl.DataFrame(all_fold_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  FINAL RESULTS: {model_name}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Calculate statistics for all metrics\n",
    "metrics = {\n",
    "    'default_accuracy': results_df['eval_default_accuracy'],\n",
    "    'default_f1_with_o': results_df['eval_default_f1_with_o'],\n",
    "    'default_f1_without_o': results_df['eval_default_f1_without_o'],\n",
    "    'strict_accuracy': results_df['eval_strict_accuracy'],\n",
    "    'strict_f1_with_o': results_df['eval_strict_f1_with_o'],\n",
    "    'strict_f1_without_o': results_df['eval_strict_f1_without_o'],\n",
    "}\n",
    "\n",
    "print(\"DEFAULT EVALUATION (Entity Type Only):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Accuracy:          {metrics['default_accuracy'].mean():.4f} ± {metrics['default_accuracy'].std():.4f}\")\n",
    "print(f\"  F1-Macro (with O): {metrics['default_f1_with_o'].mean():.4f} ± {metrics['default_f1_with_o'].std():.4f}\")\n",
    "print(f\"  F1-Macro (no O):   {metrics['default_f1_without_o'].mean():.4f} ± {metrics['default_f1_without_o'].std():.4f}\")\n",
    "\n",
    "print(\"\\nSTRICT EVALUATION (Full BIO Tags):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Accuracy:          {metrics['strict_accuracy'].mean():.4f} ± {metrics['strict_accuracy'].std():.4f}\")\n",
    "print(f\"  F1-Macro (with O): {metrics['strict_f1_with_o'].mean():.4f} ± {metrics['strict_f1_with_o'].std():.4f}\")\n",
    "print(f\"  F1-Macro (no O):   {metrics['strict_f1_without_o'].mean():.4f} ± {metrics['strict_f1_without_o'].std():.4f}\")\n",
    "\n",
    "\n",
    "csv_path = f\"{output_base}/results.csv\"\n",
    "results_df.write_csv(csv_path)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  Detailed results saved to: {csv_path}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_data = {\n",
    "    'model': [model_name],\n",
    "    'n_folds': [N_FOLDS],\n",
    "    'default_accuracy_mean': [metrics['default_accuracy'].mean()],\n",
    "    'default_accuracy_std': [metrics['default_accuracy'].std()],\n",
    "    'default_f1_with_o_mean': [metrics['default_f1_with_o'].mean()],\n",
    "    'default_f1_with_o_std': [metrics['default_f1_with_o'].std()],\n",
    "    'default_f1_without_o_mean': [metrics['default_f1_without_o'].mean()],\n",
    "    'default_f1_without_o_std': [metrics['default_f1_without_o'].std()],\n",
    "    'strict_accuracy_mean': [metrics['strict_accuracy'].mean()],\n",
    "    'strict_accuracy_std': [metrics['strict_accuracy'].std()],\n",
    "    'strict_f1_with_o_mean': [metrics['strict_f1_with_o'].mean()],\n",
    "    'strict_f1_with_o_std': [metrics['strict_f1_with_o'].std()],\n",
    "    'strict_f1_without_o_mean': [metrics['strict_f1_without_o'].mean()],\n",
    "    'strict_f1_without_o_std': [metrics['strict_f1_without_o'].std()],\n",
    "}\n",
    "\n",
    "summary_df = pl.DataFrame(summary_data)\n",
    "summary_path = f\"{output_base}/summary.csv\"\n",
    "summary_df.write_csv(summary_path)\n",
    "print(f\"  Summary statistics saved to: {summary_path}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-project (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
