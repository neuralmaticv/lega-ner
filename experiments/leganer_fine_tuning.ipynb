{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacija prepoznavanja imenovanih entiteta\n",
    "\n",
    "**Zadatak:** fine-tuning Transformer modela za prepoznavanje imenovanih entiteta (NER) na srpskom jeziku.\n",
    "**Skup podataka:** [COMtext.SR.legal](https://raw.githubusercontent.com/ICEF-NLP/COMtext.SR/ee8c2432fb4229012a3cb396b7823639216fc3da/data/comtext.sr.legal.ijekavica.conllu)  \n",
    "**Modeli:** BERTić i SrBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available, will use CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Učitavanje i parsiranje CoNLL-U formata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conllu(file_path):\n",
    "    \"\"\"\n",
    "    Parse CoNLL-U format file.\n",
    "    Returns: (sentences, labels) where each is a list of lists.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip comments and blank lines (end of sentence)\n",
    "            if line.startswith(\"#\") or not line:\n",
    "                if current_tokens:\n",
    "                    sentences.append(current_tokens)\n",
    "                    labels.append(current_labels)\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "                continue\n",
    "            \n",
    "            # Parse token line: ID FORM LEMMA POS NER\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 5 and parts[0].isdigit():\n",
    "                token = parts[1]       # Column 2: word form\n",
    "                ner_tag = parts[4]     # Column 5: NER tag\n",
    "                current_tokens.append(token)\n",
    "                current_labels.append(ner_tag)\n",
    "        \n",
    "        # Don't forget last sentence\n",
    "        if current_tokens:\n",
    "            sentences.append(current_tokens)\n",
    "            labels.append(current_labels)\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "# Load data\n",
    "data_path = Path(\"../data/comtext.sr.legal.ijekavica.conllu\")\n",
    "sentences, labels = parse_conllu(data_path)\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sentences\")\n",
    "print(f\"Total tokens: {sum(len(s) for s in sentences)}\")\n",
    "print(\"\\nExample sentence 1:\")\n",
    "print(f\"Tokens: {sentences[0][:10]}...\")\n",
    "print(f\"Labels: {labels[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza distribucije labela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set()\n",
    "for label_seq in labels:\n",
    "    all_labels.update(label_seq)\n",
    "\n",
    "unique_labels = sorted(list(all_labels))\n",
    "print(f\"Total unique labels: {len(unique_labels)}\")\n",
    "print(f\"\\nAll labels:\\n{unique_labels}\")\n",
    "\n",
    "# Count occurrences\n",
    "label_counts = {}\n",
    "for label_seq in labels:\n",
    "    for label in label_seq:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "# Show top 10 most frequent\n",
    "print(\"\\nTop 10 most frequent labels:\")\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  {label:8}: {count:6,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreiranje mapiranja labela za model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Created mappings for {len(label2id)} labels\")\n",
    "print(\"\\nFirst 10 label mappings:\")\n",
    "for label, idx in list(label2id.items())[:10]:\n",
    "    print(f\"  {label:10s} -> {idx}\")\n",
    "\n",
    "print(f\"\\nTest mapping: 'B-PER' -> {label2id['B-PER']} -> '{id2label[label2id['B-PER']]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Učitavanje modela i tokenizatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"classla/bcms-bertic\"\n",
    "\n",
    "print(f\"Loading tokenizer from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loading model from {model_name}...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True \n",
    ")\n",
    "\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Number of labels: {model.num_labels}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = sentences[0][:5]\n",
    "test_tokens = tokenizer(test_sentence, is_split_into_words=True, truncation=True)\n",
    "print(\"\\nTest tokenization:\")\n",
    "print(f\"Original tokens: {test_sentence}\")\n",
    "print(f\"Tokenized IDs: {test_tokens['input_ids'][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacija i poravnanje labela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    Tokenize text and align labels with subword tokens.\n",
    "    \n",
    "    Args:\n",
    "        examples: Dict with 'tokens' and 'ner_tags' keys\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        label2id: Label to ID mapping\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized inputs with aligned labels\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens (CLS, SEP, PAD) get -100\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # First subword of a word gets the label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label_seq[word_idx]])\n",
    "            # Subsequent subwords get -100 (ignored)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Test on first sentence\n",
    "test_example = {\n",
    "    'tokens': [sentences[0]],\n",
    "    'ner_tags': [labels[0]]\n",
    "}\n",
    "\n",
    "test_result = tokenize_and_align_labels(test_example, tokenizer, label2id)\n",
    "\n",
    "print(\"\\nTest alignment on sentence 1:\")\n",
    "print(f\"Original tokens ({len(sentences[0])}): {sentences[0][:8]}...\")\n",
    "print(f\"Original labels ({len(labels[0])}): {labels[0][:8]}...\")\n",
    "print(f\"Tokenized IDs ({len(test_result['input_ids'][0])}): {test_result['input_ids'][0][:12]}...\")\n",
    "print(f\"Aligned labels ({len(test_result['labels'][0])}): {test_result['labels'][0][:12]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priprema podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"tokens\": sentences,\n",
    "    \"ner_tags\": labels\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} sentences\")\n",
    "print(\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "print(\"\\nFirst example:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices\n",
    "train_indices, eval_indices = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create train and eval datasets\n",
    "train_dataset = dataset.select(train_indices)\n",
    "eval_dataset = dataset.select(eval_indices)\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)} sentences\")\n",
    "print(f\"Eval set:  {len(eval_dataset)} sentences\")\n",
    "print(f\"\\nSplit ratio: {len(train_dataset)/len(dataset)*100:.1f}% train / {len(eval_dataset)/len(dataset)*100:.1f}% eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing training data...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer, label2id),\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenizing evaluation data...\")\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer, label2id),\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenized train dataset: {len(tokenized_train)} examples\")\n",
    "print(f\"Tokenized eval dataset: {len(tokenized_eval)} examples\")\n",
    "print(\"\\nTokenized example:\")\n",
    "print(f\"  Input IDs length: {len(tokenized_train[0]['input_ids'])}\")\n",
    "print(f\"  Labels length: {len(tokenized_train[0]['labels'])}\")\n",
    "print(f\"  First 15 labels: {tokenized_train[0]['labels'][:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator (batching and padding)\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Data collator created - will pad sequences to batch max length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrike za evaluaciju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_bio_prefix(labels):\n",
    "    \"\"\"Convert B-PER, I-PER → PER (entity type only)\"\"\"\n",
    "    stripped = []\n",
    "    for label in labels:\n",
    "        if label == 'O':\n",
    "            stripped.append('O')\n",
    "        else:\n",
    "            # Remove B- or I- prefix\n",
    "            entity_type = label.split('-', 1)[1] if '-' in label else label\n",
    "            stripped.append(entity_type)\n",
    "    return stripped\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for model predictions.\n",
    "    This gets called automatically during evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = pred\n",
    "    \n",
    "    # Get predicted label IDs (argmax over logits)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Flatten and remove ignored indices (-100)\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] != -100:\n",
    "                true_labels.append(id2label[labels[i][j]])\n",
    "                pred_labels.append(id2label[predictions[i][j]])\n",
    "    \n",
    "    # Convert to arrays\n",
    "    y_true = np.array(true_labels)\n",
    "    y_pred = np.array(pred_labels)\n",
    "    \n",
    "    # DEFAULT EVALUATION (entity type only)\n",
    "    y_true_default = strip_bio_prefix(y_true)\n",
    "    y_pred_default = strip_bio_prefix(y_pred)\n",
    "    \n",
    "    default_acc = accuracy_score(y_true_default, y_pred_default)\n",
    "    \n",
    "    unique_labels_default = sorted(set(y_true_default) | set(y_pred_default))\n",
    "    entity_labels_default = [l for l in unique_labels_default if l != 'O']\n",
    "    \n",
    "    default_f1_with_o = f1_score(y_true_default, y_pred_default, labels=unique_labels_default, average='macro', zero_division=0)\n",
    "    default_f1_without_o = f1_score(y_true_default, y_pred_default, labels=entity_labels_default, average='macro', zero_division=0)\n",
    "    \n",
    "    # STRICT EVALUATION (full BIO tags)\n",
    "    strict_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    unique_labels = sorted(set(y_true) | set(y_pred))\n",
    "    entity_labels = [l for l in unique_labels if l != 'O']\n",
    "    \n",
    "    strict_f1_with_o = f1_score(y_true, y_pred, labels=unique_labels, average='macro', zero_division=0)\n",
    "    strict_f1_without_o = f1_score(y_true, y_pred, labels=entity_labels, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        # Default mode\n",
    "        'default_accuracy': default_acc,\n",
    "        'default_f1_with_o': default_f1_with_o,\n",
    "        'default_f1_without_o': default_f1_without_o,\n",
    "\n",
    "        # Strict mode\n",
    "        'strict_accuracy': strict_acc,\n",
    "        'strict_f1_with_o': strict_f1_with_o,\n",
    "        'strict_f1_without_o': strict_f1_without_o,\n",
    "    }\n",
    "\n",
    "print(\"Evaluation metrics function created\")\n",
    "print(\"  - Default mode: Entity type only\")\n",
    "print(\"  - Strict mode: Full BIO tag matching\")\n",
    "print(\"  - Metrics: Accuracy, F1-Macro (with/without O)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracija za treniranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = f\"../outputs/models/bertic_ner_{timestamp}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"strict_f1_without_o\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"../outputs/logs/{timestamp}\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")\n",
    "print(\"\\nKey settings:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")\n",
    "print(f\"  Output: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")\n",
    "print(f\"Ready to train on {len(tokenized_train)} training examples\")\n",
    "print(f\"Will evaluate on {len(tokenized_eval)} eval examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treniranje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTraining metrics:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nRunning final evaluation...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"DEFAULT EVALUATION (entity type only):\")\n",
    "print(f\"  Accuracy:           {eval_metrics['eval_default_accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro (with O):  {eval_metrics['eval_default_f1_with_o']:.4f}\")\n",
    "print(f\"  F1-Macro (no O):    {eval_metrics['eval_default_f1_without_o']:.4f}\")\n",
    "print()\n",
    "print(\"STRICT EVALUATION (full BIO tags):\")\n",
    "print(f\"  Accuracy:           {eval_metrics['eval_strict_accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro (with O):  {eval_metrics['eval_strict_f1_with_o']:.4f}\")\n",
    "print(f\"  F1-Macro (no O):    {eval_metrics['eval_strict_f1_without_o']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-project (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
