{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacija prepoznavanja imenovanih entiteta\n",
    "\n",
    "**Zadatak:** fine-tuning Transformer modela za prepoznavanje imenovanih entiteta (NER) na srpskom jeziku.  \n",
    "**Skup podataka:** [COMtext.SR.legal](https://raw.githubusercontent.com/ICEF-NLP/COMtext.SR/ee8c2432fb4229012a3cb396b7823639216fc3da/data/comtext.sr.legal.ijekavica.conllu)  \n",
    "**Modeli:** BERTić i SrBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.57.3\n",
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU: NVIDIA L40S\n",
      "GPU memory: 47.67 GB\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available, will use CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Učitavanje i parsiranje CoNLL-U formata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4762 sentences\n",
      "Total tokens: 105470\n",
      "\n",
      "Example sentence 1:\n",
      "Tokens: ['Trans', 'Impex', 'Trade', 'd.o.o.', 'Bul.', 'Vojvode', 'Stepe', '123/2', '21000', 'Novi']...\n",
      "Labels: ['B-COM', 'I-COM', 'I-COM', 'I-COM', 'B-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'I-ADR']...\n"
     ]
    }
   ],
   "source": [
    "def parse_conllu(file_path):\n",
    "    \"\"\"\n",
    "    Parse CoNLL-U format file.\n",
    "    Returns: (sentences, labels) where each is a list of lists.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    \n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip comments and blank lines (end of sentence)\n",
    "            if line.startswith(\"#\") or not line:\n",
    "                if current_tokens:\n",
    "                    sentences.append(current_tokens)\n",
    "                    labels.append(current_labels)\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "                continue\n",
    "            \n",
    "            # Parse token line: ID FORM LEMMA POS NER\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 5 and parts[0].isdigit():\n",
    "                token = parts[1]       # Column 2: word form\n",
    "                ner_tag = parts[4]     # Column 5: NER tag\n",
    "                current_tokens.append(token)\n",
    "                current_labels.append(ner_tag)\n",
    "        \n",
    "        # Don't forget last sentence\n",
    "        if current_tokens:\n",
    "            sentences.append(current_tokens)\n",
    "            labels.append(current_labels)\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "# Load data\n",
    "data_path = Path(\"../data/comtext.sr.legal.ijekavica.conllu\")\n",
    "sentences, labels = parse_conllu(data_path)\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sentences\")\n",
    "print(f\"Total tokens: {sum(len(s) for s in sentences)}\")\n",
    "print(\"\\nExample sentence 1:\")\n",
    "print(f\"Tokens: {sentences[0][:10]}...\")\n",
    "print(f\"Labels: {labels[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza distribucije labela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique labels: 35\n",
      "\n",
      "All labels:\n",
      "['B-ADR', 'B-COM', 'B-CONTACT', 'B-COURT', 'B-DATE', 'B-IDCOM', 'B-IDOTH', 'B-IDPER', 'B-IDTAX', 'B-INST', 'B-LAW', 'B-MISC', 'B-MONEY', 'B-NUMACC', 'B-NUMCAR', 'B-NUMDOC', 'B-NUMPLOT', 'B-ORGOTH', 'B-PER', 'B-REF', 'B-TOP', 'I-ADR', 'I-COM', 'I-COURT', 'I-DATE', 'I-INST', 'I-LAW', 'I-MISC', 'I-MONEY', 'I-NUMCAR', 'I-ORGOTH', 'I-PER', 'I-REF', 'I-TOP', 'O']\n",
      "\n",
      "Top 10 most frequent labels:\n",
      "  O       : 91,357\n",
      "  I-LAW   :  3,229\n",
      "  I-REF   :  2,161\n",
      "  I-ADR   :  1,211\n",
      "  I-DATE  :  1,090\n",
      "  B-PER   :    694\n",
      "  I-INST  :    693\n",
      "  I-PER   :    617\n",
      "  I-COM   :    432\n",
      "  B-LAW   :    395\n"
     ]
    }
   ],
   "source": [
    "all_labels = set()\n",
    "for label_seq in labels:\n",
    "    all_labels.update(label_seq)\n",
    "\n",
    "unique_labels = sorted(list(all_labels))\n",
    "print(f\"Total unique labels: {len(unique_labels)}\")\n",
    "print(f\"\\nAll labels:\\n{unique_labels}\")\n",
    "\n",
    "# Count occurrences\n",
    "label_counts = {}\n",
    "for label_seq in labels:\n",
    "    for label in label_seq:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "# Show top 10 most frequent\n",
    "print(\"\\nTop 10 most frequent labels:\")\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  {label:8}: {count:6,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreiranje mapiranja labela za model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mappings for 35 labels\n",
      "\n",
      "First 10 label mappings:\n",
      "  B-ADR      -> 0\n",
      "  B-COM      -> 1\n",
      "  B-CONTACT  -> 2\n",
      "  B-COURT    -> 3\n",
      "  B-DATE     -> 4\n",
      "  B-IDCOM    -> 5\n",
      "  B-IDOTH    -> 6\n",
      "  B-IDPER    -> 7\n",
      "  B-IDTAX    -> 8\n",
      "  B-INST     -> 9\n",
      "\n",
      "Test mapping: 'B-PER' -> 18 -> 'B-PER'\n"
     ]
    }
   ],
   "source": [
    "# Label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Created mappings for {len(label2id)} labels\")\n",
    "print(\"\\nFirst 10 label mappings:\")\n",
    "for label, idx in list(label2id.items())[:10]:\n",
    "    print(f\"  {label:10s} -> {idx}\")\n",
    "\n",
    "print(f\"\\nTest mapping: 'B-PER' -> {label2id['B-PER']} -> '{id2label[label2id['B-PER']]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Učitavanje modela i tokenizatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from classla/bcms-bertic...\n",
      "Loading model from classla/bcms-bertic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: ElectraForTokenClassification\n",
      "Number of labels: 35\n",
      "Model parameters: 110,053,667\n"
     ]
    }
   ],
   "source": [
    "model_name = \"classla/bcms-bertic\"\n",
    "\n",
    "print(f\"Loading tokenizer from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loading model from {model_name}...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True \n",
    ")\n",
    "\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Number of labels: {model.num_labels}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test tokenization:\n",
      "Original tokens: ['Trans', 'Impex', 'Trade', 'd.o.o.', 'Bul.']\n",
      "Tokenized IDs: [2, 21006, 12906, 2042, 1032, 18278, 72, 18, 83, 18]...\n"
     ]
    }
   ],
   "source": [
    "test_sentence = sentences[0][:5]\n",
    "test_tokens = tokenizer(test_sentence, is_split_into_words=True, truncation=True)\n",
    "print(\"\\nTest tokenization:\")\n",
    "print(f\"Original tokens: {test_sentence}\")\n",
    "print(f\"Tokenized IDs: {test_tokens['input_ids'][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacija i poravnanje labela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test alignment on sentence 1:\n",
      "Original tokens (11): ['Trans', 'Impex', 'Trade', 'd.o.o.', 'Bul.', 'Vojvode', 'Stepe', '123/2']...\n",
      "Original labels (11): ['B-COM', 'I-COM', 'I-COM', 'I-COM', 'B-ADR', 'I-ADR', 'I-ADR', 'I-ADR']...\n",
      "Tokenized IDs (27): [2, 21006, 12906, 2042, 1032, 18278, 72, 18, 83, 18, 83, 18]...\n",
      "Aligned labels (27): [-100, 1, 22, -100, -100, 22, 22, -100, -100, -100, -100, -100]...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    Tokenize text and align labels with subword tokens.\n",
    "    \n",
    "    Args:\n",
    "        examples: Dict with 'tokens' and 'ner_tags' keys\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        label2id: Label to ID mapping\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized inputs with aligned labels\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens (CLS, SEP, PAD) get -100\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # First subword of a word gets the label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label_seq[word_idx]])\n",
    "            # Subsequent subwords get -100 (ignored)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Test on first sentence\n",
    "test_example = {\n",
    "    'tokens': [sentences[0]],\n",
    "    'ner_tags': [labels[0]]\n",
    "}\n",
    "\n",
    "test_result = tokenize_and_align_labels(test_example, tokenizer, label2id)\n",
    "\n",
    "print(\"\\nTest alignment on sentence 1:\")\n",
    "print(f\"Original tokens ({len(sentences[0])}): {sentences[0][:8]}...\")\n",
    "print(f\"Original labels ({len(labels[0])}): {labels[0][:8]}...\")\n",
    "print(f\"Tokenized IDs ({len(test_result['input_ids'][0])}): {test_result['input_ids'][0][:12]}...\")\n",
    "print(f\"Aligned labels ({len(test_result['labels'][0])}): {test_result['labels'][0][:12]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priprema podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 4762 sentences\n",
      "\n",
      "Dataset structure:\n",
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 4762\n",
      "})\n",
      "\n",
      "First example:\n",
      "{'tokens': ['Trans', 'Impex', 'Trade', 'd.o.o.', 'Bul.', 'Vojvode', 'Stepe', '123/2', '21000', 'Novi', 'Sad'], 'ner_tags': ['B-COM', 'I-COM', 'I-COM', 'I-COM', 'B-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'I-ADR']}\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    \"tokens\": sentences,\n",
    "    \"ner_tags\": labels\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "print(f\"Created dataset with {len(dataset)} sentences\")\n",
    "print(\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "print(\"\\nFirst example:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 4285 sentences\n",
      "Eval set:  477 sentences\n",
      "\n",
      "Split ratio: 90.0% train / 10.0% eval\n"
     ]
    }
   ],
   "source": [
    "# Split indices\n",
    "train_indices, eval_indices = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create train and eval datasets\n",
    "train_dataset = dataset.select(train_indices)\n",
    "eval_dataset = dataset.select(eval_indices)\n",
    "\n",
    "print(f\"Train set: {len(train_dataset)} sentences\")\n",
    "print(f\"Eval set:  {len(eval_dataset)} sentences\")\n",
    "print(f\"\\nSplit ratio: {len(train_dataset)/len(dataset)*100:.1f}% train / {len(eval_dataset)/len(dataset)*100:.1f}% eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4285/4285 [00:00<00:00, 11071.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing evaluation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 477/477 [00:00<00:00, 10252.71 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized train dataset: 4285 examples\n",
      "Tokenized eval dataset: 477 examples\n",
      "\n",
      "Tokenized example:\n",
      "  Input IDs length: 35\n",
      "  Labels length: 35\n",
      "  First 15 labels: [-100, 34, 34, 34, -100, -100, 34, 34, 34, 34, -100, 34, 34, 34, 34]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing training data...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer, label2id),\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenizing evaluation data...\")\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer, label2id),\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenized train dataset: {len(tokenized_train)} examples\")\n",
    "print(f\"Tokenized eval dataset: {len(tokenized_eval)} examples\")\n",
    "print(\"\\nTokenized example:\")\n",
    "print(f\"  Input IDs length: {len(tokenized_train[0]['input_ids'])}\")\n",
    "print(f\"  Labels length: {len(tokenized_train[0]['labels'])}\")\n",
    "print(f\"  First 15 labels: {tokenized_train[0]['labels'][:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator created - will pad sequences to batch max length\n"
     ]
    }
   ],
   "source": [
    "# Create data collator (batching and padding)\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Data collator created - will pad sequences to batch max length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrike za evaluaciju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics function created\n",
      "  - Default mode: Entity type only\n",
      "  - Strict mode: Full BIO tag matching\n",
      "  - Metrics: Accuracy, F1-Macro (with/without O)\n"
     ]
    }
   ],
   "source": [
    "def strip_bio_prefix(labels):\n",
    "    \"\"\"Convert B-PER, I-PER → PER (entity type only)\"\"\"\n",
    "    stripped = []\n",
    "    for label in labels:\n",
    "        if label == 'O':\n",
    "            stripped.append('O')\n",
    "        else:\n",
    "            # Remove B- or I- prefix\n",
    "            entity_type = label.split('-', 1)[1] if '-' in label else label\n",
    "            stripped.append(entity_type)\n",
    "    return stripped\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for model predictions.\n",
    "    This gets called automatically during evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = pred\n",
    "    \n",
    "    # Get predicted label IDs (argmax over logits)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Flatten and remove ignored indices (-100)\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] != -100:\n",
    "                true_labels.append(id2label[labels[i][j]])\n",
    "                pred_labels.append(id2label[predictions[i][j]])\n",
    "    \n",
    "    # Convert to arrays\n",
    "    y_true = np.array(true_labels)\n",
    "    y_pred = np.array(pred_labels)\n",
    "    \n",
    "    # DEFAULT EVALUATION (entity type only)\n",
    "    y_true_default = strip_bio_prefix(y_true)\n",
    "    y_pred_default = strip_bio_prefix(y_pred)\n",
    "    \n",
    "    default_acc = accuracy_score(y_true_default, y_pred_default)\n",
    "    \n",
    "    unique_labels_default = sorted(set(y_true_default) | set(y_pred_default))\n",
    "    entity_labels_default = [l for l in unique_labels_default if l != 'O']\n",
    "    \n",
    "    default_f1_with_o = f1_score(y_true_default, y_pred_default, labels=unique_labels_default, average='macro', zero_division=0)\n",
    "    default_f1_without_o = f1_score(y_true_default, y_pred_default, labels=entity_labels_default, average='macro', zero_division=0)\n",
    "    \n",
    "    # STRICT EVALUATION (full BIO tags)\n",
    "    strict_acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    unique_labels = sorted(set(y_true) | set(y_pred))\n",
    "    entity_labels = [l for l in unique_labels if l != 'O']\n",
    "    \n",
    "    strict_f1_with_o = f1_score(y_true, y_pred, labels=unique_labels, average='macro', zero_division=0)\n",
    "    strict_f1_without_o = f1_score(y_true, y_pred, labels=entity_labels, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        # Default mode\n",
    "        'default_accuracy': default_acc,\n",
    "        'default_f1_with_o': default_f1_with_o,\n",
    "        'default_f1_without_o': default_f1_without_o,\n",
    "\n",
    "        # Strict mode\n",
    "        'strict_accuracy': strict_acc,\n",
    "        'strict_f1_with_o': strict_f1_with_o,\n",
    "        'strict_f1_without_o': strict_f1_without_o,\n",
    "    }\n",
    "\n",
    "print(\"Evaluation metrics function created\")\n",
    "print(\"  - Default mode: Entity type only\")\n",
    "print(\"  - Strict mode: Full BIO tag matching\")\n",
    "print(\"  - Metrics: Accuracy, F1-Macro (with/without O)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracija za treniranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured\n",
      "\n",
      "Key settings:\n",
      "  Epochs: 20\n",
      "  Train batch size: 16\n",
      "  Learning rate: 5e-05\n",
      "  FP16: True\n",
      "  Output: ../outputs/models/bertic_ner_20251222_153632\n"
     ]
    }
   ],
   "source": [
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = f\"../outputs/models/bertic_ner_{timestamp}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"strict_f1_without_o\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"../outputs/logs/{timestamp}\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")\n",
    "print(\"\\nKey settings:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")\n",
    "print(f\"  Output: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on device: cuda:0\n",
      "Ready to train on 4285 training examples\n",
      "Will evaluate on 477 eval examples\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")\n",
    "print(f\"Ready to train on {len(tokenized_train)} training examples\")\n",
    "print(f\"Will evaluate on {len(tokenized_eval)} eval examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treniranje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5360' max='5360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5360/5360 06:34, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Default Accuracy</th>\n",
       "      <th>Default F1 With O</th>\n",
       "      <th>Default F1 Without O</th>\n",
       "      <th>Strict Accuracy</th>\n",
       "      <th>Strict F1 With O</th>\n",
       "      <th>Strict F1 Without O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.327294</td>\n",
       "      <td>0.941722</td>\n",
       "      <td>0.298347</td>\n",
       "      <td>0.260298</td>\n",
       "      <td>0.925480</td>\n",
       "      <td>0.172392</td>\n",
       "      <td>0.145364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.116409</td>\n",
       "      <td>0.984714</td>\n",
       "      <td>0.533602</td>\n",
       "      <td>0.507961</td>\n",
       "      <td>0.979364</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.521767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.077266</td>\n",
       "      <td>0.986242</td>\n",
       "      <td>0.615649</td>\n",
       "      <td>0.595740</td>\n",
       "      <td>0.984714</td>\n",
       "      <td>0.664808</td>\n",
       "      <td>0.654191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.041934</td>\n",
       "      <td>0.991497</td>\n",
       "      <td>0.650992</td>\n",
       "      <td>0.632788</td>\n",
       "      <td>0.990542</td>\n",
       "      <td>0.708347</td>\n",
       "      <td>0.699040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.031093</td>\n",
       "      <td>0.992548</td>\n",
       "      <td>0.681580</td>\n",
       "      <td>0.664970</td>\n",
       "      <td>0.991306</td>\n",
       "      <td>0.746035</td>\n",
       "      <td>0.737934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.058189</td>\n",
       "      <td>0.990637</td>\n",
       "      <td>0.681598</td>\n",
       "      <td>0.665048</td>\n",
       "      <td>0.989682</td>\n",
       "      <td>0.770166</td>\n",
       "      <td>0.762879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.053188</td>\n",
       "      <td>0.992835</td>\n",
       "      <td>0.754206</td>\n",
       "      <td>0.741440</td>\n",
       "      <td>0.991975</td>\n",
       "      <td>0.813981</td>\n",
       "      <td>0.808085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.062291</td>\n",
       "      <td>0.990828</td>\n",
       "      <td>0.773057</td>\n",
       "      <td>0.761328</td>\n",
       "      <td>0.989777</td>\n",
       "      <td>0.826642</td>\n",
       "      <td>0.821181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.051725</td>\n",
       "      <td>0.991975</td>\n",
       "      <td>0.859491</td>\n",
       "      <td>0.852284</td>\n",
       "      <td>0.991115</td>\n",
       "      <td>0.883978</td>\n",
       "      <td>0.880350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.046417</td>\n",
       "      <td>0.993312</td>\n",
       "      <td>0.927806</td>\n",
       "      <td>0.923947</td>\n",
       "      <td>0.992452</td>\n",
       "      <td>0.928779</td>\n",
       "      <td>0.926496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.992452</td>\n",
       "      <td>0.916856</td>\n",
       "      <td>0.912663</td>\n",
       "      <td>0.991688</td>\n",
       "      <td>0.919216</td>\n",
       "      <td>0.916722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.056411</td>\n",
       "      <td>0.992930</td>\n",
       "      <td>0.874732</td>\n",
       "      <td>0.868298</td>\n",
       "      <td>0.991975</td>\n",
       "      <td>0.889880</td>\n",
       "      <td>0.886425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.052033</td>\n",
       "      <td>0.993312</td>\n",
       "      <td>0.921809</td>\n",
       "      <td>0.917852</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.922413</td>\n",
       "      <td>0.920008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.059759</td>\n",
       "      <td>0.992166</td>\n",
       "      <td>0.968395</td>\n",
       "      <td>0.966847</td>\n",
       "      <td>0.991593</td>\n",
       "      <td>0.949166</td>\n",
       "      <td>0.947596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.064384</td>\n",
       "      <td>0.992644</td>\n",
       "      <td>0.926727</td>\n",
       "      <td>0.923050</td>\n",
       "      <td>0.991688</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>0.927026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.069109</td>\n",
       "      <td>0.992548</td>\n",
       "      <td>0.969849</td>\n",
       "      <td>0.968360</td>\n",
       "      <td>0.991593</td>\n",
       "      <td>0.951290</td>\n",
       "      <td>0.949778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.065448</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.918343</td>\n",
       "      <td>0.914219</td>\n",
       "      <td>0.991784</td>\n",
       "      <td>0.919927</td>\n",
       "      <td>0.917451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.064918</td>\n",
       "      <td>0.992166</td>\n",
       "      <td>0.917637</td>\n",
       "      <td>0.913493</td>\n",
       "      <td>0.991402</td>\n",
       "      <td>0.920343</td>\n",
       "      <td>0.917891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.059011</td>\n",
       "      <td>0.992452</td>\n",
       "      <td>0.919672</td>\n",
       "      <td>0.915626</td>\n",
       "      <td>0.991688</td>\n",
       "      <td>0.922118</td>\n",
       "      <td>0.919718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.056802</td>\n",
       "      <td>0.992548</td>\n",
       "      <td>0.920536</td>\n",
       "      <td>0.916536</td>\n",
       "      <td>0.991784</td>\n",
       "      <td>0.922786</td>\n",
       "      <td>0.920406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training completed!\n",
      "======================================================================\n",
      "\n",
      "Training metrics:\n",
      "  train_runtime: 394.7971\n",
      "  train_samples_per_second: 217.074\n",
      "  train_steps_per_second: 13.577\n",
      "  total_flos: 4610805161720160.0\n",
      "  train_loss: 0.09054291141427942\n",
      "  epoch: 20.0\n",
      "\n",
      "Running final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Results:\n",
      "----------------------------------------------------------------------\n",
      "DEFAULT EVALUATION (entity type only):\n",
      "  Accuracy:           0.9925\n",
      "  F1-Macro (with O):  0.9698\n",
      "  F1-Macro (no O):    0.9684\n",
      "\n",
      "STRICT EVALUATION (full BIO tags):\n",
      "  Accuracy:           0.9916\n",
      "  F1-Macro (with O):  0.9513\n",
      "  F1-Macro (no O):    0.9498\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTraining metrics:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nRunning final evaluation...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"DEFAULT EVALUATION (entity type only):\")\n",
    "print(f\"  Accuracy:           {eval_metrics['eval_default_accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro (with O):  {eval_metrics['eval_default_f1_with_o']:.4f}\")\n",
    "print(f\"  F1-Macro (no O):    {eval_metrics['eval_default_f1_without_o']:.4f}\")\n",
    "print()\n",
    "print(\"STRICT EVALUATION (full BIO tags):\")\n",
    "print(f\"  Accuracy:           {eval_metrics['eval_strict_accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro (with O):  {eval_metrics['eval_strict_f1_with_o']:.4f}\")\n",
    "print(f\"  F1-Macro (no O):    {eval_metrics['eval_strict_f1_without_o']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-project (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
